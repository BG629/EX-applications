## Info for Practices:

# Practice 1:
# Practice 2:
# Practice 3:

crosstab
describe
quantile
IQR
groupby

# Practice 4 - charts:

histogram
boxplot
Scatterplot
Matrix plot

# Practice 5 - linear regression:

simple liner regression
multy-var liner regression
MSE
RMSE
R^2

# Practice 6 - linear regression:

Train Test Split
Polynomial regression

# Practice 7 - Logistic Regression:

pd.get_dummies()
pd.concat(initial Dataset, new_dummy var, new_dummy var, axis=1)

Confusion Matrix (https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62)

classification_report (recall/f1/presicion) 

ROC


# Practice 8:
# Practice 9:
# Practice 10:




******************************************************************

# earthquakes (var) goes up:

Recall +
presicion -

# earthquakes (var) goes down:

Recall -
presicion +


# Information Gain (IG):

```json
the addition of inforamtion of a feature on the target variable
```

# Random Forest:

leads to better model performance because it decreases the model’s variance, without increasing the bias


# Accuracy:

value of correct answers base on the whole model (correct and incorrect)

# Recall:

how much did i perdict out of all earthquakes that happend
(60 times out of 100 earthquakes)


# 𝜃0 == bias


# Scaling Methods: (best to worst)

1. quartile standartization - requeiers sort of the data to find median (effective only with lots of data)
2. Standartization
3. Normalization

The more the dataset is smaller, there is more impact on outliers.
The more the dataset is bigger, there is less impact on outliers.

example:
df_a: 1000 samples, 10 features
df_b: 100,000 samples, 100 features

df_a -> quartile standartization
df_b -> Standartization


# OverFitting:
```json
High Variance
Low Bias
```

# UnderFitting:
```json
Low Variance
High Bias
```

***************************************************
## lecterures:

https://drive.google.com/drive/u/0/folders/1QF5EsAOSKQNr1oumy93HCKspBTey5wAl

connect with mindad

1. map redice
2. scaling techniques
